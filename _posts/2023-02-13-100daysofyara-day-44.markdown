---
layout: post
title: "100 Days of YARA - Day 44"
date: 2023-02-13 00:00:00 -0000
categories: yara
---

# Math Module - Randomness Functions Efficiency Comparison
So we've got three different measures of "randomness" in the math module: entropy, Monte Carlo pi, and serial correlation. So, which one should you use?

Disregarding the mathematical nuances of each function, a potential way of answering this is by determing which is the most efficient function to use. Using Florian Roth and Arnim Rupp's tool for measuring YARA performance, [Panopticon](https://github.com/Neo23x0/panopticon), I ran each function against a bunch of files I grabbed from `C:\Windows\system32` to get the following output:
```
[WARNING] Rule "entropy" slows down a search with 50 rules by 12.8011 % (Measured by best of 3 runs)
[WARNING] Rule "monte_carlo" slows down a search with 50 rules by 35.2246 % (Measured by best of 3 runs)
[WARNING] Rule "serial_correlation" slows down a search with 50 rules by 20.0926 % (Measured by best of 3 runs)
```

While this is somewhat anectodal, and results will vary between the baseline rules you use/samples you target/general reruns, this run of the tool shows that entropy performs the best, followed by serial correlation then Monte Carlo pi. 

This is quite a gratifying result, as I (and many others) have used entropy as the main way of measuring "randomness" in YARA, so to know this is likely the most efficient of the functions means we don't have to change too much in our approach.

However, hopefully what these posts/discussion around them have shown is that there is value in looking at these functions more closely to figure out how best they can be used! I know I will personally be trying to find new ways to use Monte Carlo pi and serial correlation in my rules going forward.