---
layout: post
title: "100 Days of YARA - Day 42"
date: 2023-02-11 00:00:00 -0000
categories: yara
---

# Math Module - Monte Carlo Pi
The entropy function is the only measure of "randomness" in the math module (as an aside, see this Twitter thread started by [@notareverser](https://twitter.com/notareverser) that gives a more nuanced look on what the entropy is measuring: [https://twitter.com/notareverser/status/1623842328044527616](https://twitter.com/notareverser/status/1623842328044527616)). Another one to consider is the `math.monte_carlo_pi` [function](https://yara.readthedocs.io/en/stable/modules/math.html#c.monte_carlo_pi).

This function will use a Monte Carlo method of [estimating the value of pi](https://academo.org/demos/estimating-pi-monte-carlo/) (`3.14159265359...`). This is done by taking the values which are input into the function, scaling them and placing them on a graph of a circle. By comparing the number of points that are inside vs. outside the circle, the value of pi can be estimated. The idea is that the more random points there are plotted in/out of the circle, the more accurate the estimation of pi will be. If the data isn't very "random" though, the estimation won't be as good.
![](/assets/2023-02-11_monte_carlo_pi_estimate.png)

The output of `math.monte_carlo_pi` is not the estimated value of pi, but rather the percentage difference between the true value of pi compared to the estimated one. As such, the lower the output of the function, the closer the estimation to pi was, and the more "random" the data was. The higher the number, the further away from pi the estimation, so the less random it was.

If I take a string that has no randomness, then we can see what a bad estimation looks like:
```
import "math"
import "console"

rule Bad_Monte_Carlo_Pi_Estimate {
    condition:
        console.log(math.monte_carlo_pi("\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00"))
}
```

In this case, the result is `0.273240`, which is approximately `(4 - pi) / pi` (i.e. the estimated value of pi with this data is the value `4` - maybe good enough for engineering, not good enough for maths!). So this can be considered a bad estimation.

If I do the same test but instead of for null I use `0xFF`:
```
import "math"
import "console"

rule Bad_Monte_Carlo_Pi_Estimate {
    condition:
        console.log(math.monte_carlo_pi("\xFF\xFF\xFF\xFF\xFF\xFF\xFF\xFF\xFF\xFF\xFF\xFF\xFF\xFF\xFF\xFF\xFF"))
}
```

Then the output for this is `1.000000` (i.e. pi would be computed to be 0, and so is 100% off the true value).

Alternatively, if I take some random data (10,000 bytes worth) and repeat this test against a file with the random data in it:
```
import "math"
import "console"

rule Monte_Carlo_Pi_Estimate {
    condition:
        console.log(math.monte_carlo_pi(0, filesize))
}
```

I get `0.019468`, i.e. much closer to zero. For context, on the same data I get the entropy result of `7.981289`.

So in short, data the appears random/compressed etc. is going to have a low value from this function as it better estimates pi, whereas as very ordered data is going to be higher in value. More experimentation is need to figure out what kind of outputs are good at "predicting" certain types of data!